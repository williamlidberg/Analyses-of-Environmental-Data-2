{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUl9OVOUn34Kirjh5KJFsl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/williamlidberg/Analyses-of-Environmental-Data-2/blob/main/modules/module_6/Assignment_6_geopandas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Geopandas\n",
        "You can think of geopandas as pandas for vector data. By vector data I mean points, lines, polygons where each feature has a corresponding row in an attribute table. This is also refered to as tabular data. You have used geopandas in previous modules but the aim of this module is to prepare you for next module which is machine learning on vector data.\n",
        "\n",
        "Start by installing geopandas and a few other packages"
      ],
      "metadata": {
        "id": "48d6zNMNnNuZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9ipcFgQmbNH"
      },
      "outputs": [],
      "source": [
        "!pip install geopandas\n",
        "!pip install matplotlib # for plotting\n",
        "!pip install folium # for data exploration\n",
        "!pip install mapclassify # for data exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading and writing data\n",
        "The best thing about geopandas is that i can handle multiple file types such as GeoPackage, GeoJSON, Shapefile. Shapefiles can only contain about 4 GB of data and since many modern datasets is much larger than that the industry is slowly moving over to GeoPackage. You can easily read all these datatypes with the geopandas command \"read_file()\""
      ],
      "metadata": {
        "id": "oG__AfY2TvvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start by downloading some data to work with"
      ],
      "metadata": {
        "id": "6j1AyOdczifq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download ditches"
      ],
      "metadata": {
        "id": "fD-AesWk4DtH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlretrieve\n",
        "url = ('https://geodata.naturvardsverket.se/nedladdning/diken/Diken_Sverige/Diken_lansvis/Diken_K.zip')\n",
        "filename = '/content/Diken_K.zip' # you need to adjust this path on your own computer if you are using anaconda.\n",
        "urlretrieve(url, filename)\n",
        "!unzip -o /content/Diken_K.zip -d /content/ditches"
      ],
      "metadata": {
        "id": "5WXVJPGVi5mJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download wetlands"
      ],
      "metadata": {
        "id": "6ZYrjUUfz0aV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlretrieve\n",
        "url = ('https://geodata.naturvardsverket.se/nedladdning/VMI/ursprunglig_digitalisering/K_Blekinge_VMI.zip')\n",
        "filename = '/content/K_Blekinge_VMI.zip' # you need to adjust this path on your own computer if you are using anaconda.\n",
        "urlretrieve(url, filename)\n",
        "!unzip -o '/content/K_Blekinge_VMI.zip' -d /content/wetlands"
      ],
      "metadata": {
        "id": "Ki2DhgDwkEKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download protected areas"
      ],
      "metadata": {
        "id": "UAo6peq_4Hvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlretrieve\n",
        "url = ('https://geodata.naturvardsverket.se/nedladdning/naturvardsregistret/biosfarsomraden.zip')\n",
        "filename = '/content/biosfarsomraden.zip' # you need to adjust this path on your own computer if you are using anaconda.\n",
        "urlretrieve(url, filename)\n",
        "!unzip -o '/content/biosfarsomraden.zip' -d /content/protected"
      ],
      "metadata": {
        "id": "VGgxvWvA4LA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read all data into geodataframes\n",
        "All file types can be read with the same command."
      ],
      "metadata": {
        "id": "0aTRfoLV0WMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "ditches = gpd.read_file('/content/ditches/Diken_K.gpkg') # Note that this is a geopackage\n",
        "wetlands = gpd.read_file('/content/wetlands/K_Blekinge_VMI_Ytor.shp')\n",
        "protected = gpd.read_file('/content/protected/BIOSFARSOMRADEN.shp')"
      ],
      "metadata": {
        "id": "qBKRuS2TxU9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also write the geodataframes to different file types. For example to save the wetlands polygons to a geojson we can specify the driver as GeoJson."
      ],
      "metadata": {
        "id": "MfrA3GvFUwom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wetlands.to_file(\"/content/wetlands.geojson\", driver=\"GeoJSON\")"
      ],
      "metadata": {
        "id": "jgUKdTd85e7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1\n",
        "Save the protected areas dataframe as a geopackage."
      ],
      "metadata": {
        "id": "dWOHv964VVRY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visalizing data\n",
        "Geopandas includes a function to quickly inspect the data if the dataset is not too big. This works for the wetland polygons but it would probably crash google colab if you did this with the ditch dataframe. This is a very neat command to remember.\n",
        "\n"
      ],
      "metadata": {
        "id": "5vIYoBQ_VhvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wetlands.explore(column=\"NVKLASS\") # move over the polygons to see the attibutes."
      ],
      "metadata": {
        "id": "auDF7-2d00UG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Selecting data by attributes\n",
        "It's quite common to be interested in parts of the dataset during an analysis. In most GIS programs you can run a tool named something along the lines \"select by attributes\". Since geopandas builds on pandas this can easily be achived with Python as well.\n",
        "\n",
        "Lets pretend that we want to investigate ditches but only forest ditches. We can then select or slice the dataframe by that attribute. In our case: Typ"
      ],
      "metadata": {
        "id": "gG2dJxXDbakN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ditches"
      ],
      "metadata": {
        "id": "cZImoen7cCFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forest_ditches = ditches[ditches[\"Typ\"] == \"Skogsdike\"] # Skogsdike is Swedish for forest ditch"
      ],
      "metadata": {
        "id": "51r_oKQHcJHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes we want to make multiple selections at the same time. This is where the & sign can be used. Let's select forest ditches that are longer than 500 meters by adding the lenght attribute to the selection."
      ],
      "metadata": {
        "id": "_DiJ8lh5db_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "forest_ditches_500m = ditches[(ditches[\"Typ\"] == \"Skogsdike\") & (ditches[\"Langd_m\"] > 500)] # Langd, or Längd is Swedish for length\n",
        "forest_ditches_500m.explore()"
      ],
      "metadata": {
        "id": "YgpiwMuheAfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2\n",
        "Make a selection on the wetlands geodataframe and awsner the question: How many wetlands have a high nature value (Högt naturvärde), an area over 10 hectares and a perimiter less than 2000 meters."
      ],
      "metadata": {
        "id": "k39GbyyRgbmg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spatial relations\n",
        "One of the most commonly used operations within GIS is to select features from one dataset that falls within the features of another dataset. This is commonly refered to as spatial join or intersect.\n",
        "\n",
        "Lets select all wetlands that fall within the protective area using geopandas."
      ],
      "metadata": {
        "id": "EwQ289Q_6okb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selected_wetlands = gpd.sjoin(wetlands, protected, op='within')\n",
        "selected_wetlands.explore(column=\"NVKLASS\")"
      ],
      "metadata": {
        "id": "4Vm6W9-J7EXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can reverse this selection by using the ~ sign."
      ],
      "metadata": {
        "id": "biizSpFD-HO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "non_intersecting_wetlands = wetlands[~wetlands.index.isin(selected_wetlands.index)]\n",
        "non_intersecting_wetlands .explore(column=\"NVKLASS\")"
      ],
      "metadata": {
        "id": "yobF4UmB-K4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The spatial join operation have many different options:\n",
        "*   contains\n",
        "*   within\n",
        "*   touches\n",
        "*   crosses\n",
        "*   overlaps\n",
        "*   equals\n",
        "\n",
        "## Task 3\n",
        "Use geopandas to select ditches that crosses the wetlands and then show them on a map."
      ],
      "metadata": {
        "id": "2bG8ylFx6iEn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Math operations\n",
        "Another common operation is to summarize the area/lenght or features that falls within other features. For example we can summarize the lenght of ditches within the wetlands."
      ],
      "metadata": {
        "id": "iIlwAKqtEezk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joined_gdf = gpd.sjoin(ditches, wetlands, how=\"inner\", op='intersects') # intersect ditches with wetlands\n",
        "joined_gdf['line_length'] = joined_gdf.geometry.length # calculate lenght of intersected ditches\n",
        "summary = joined_gdf.groupby('NVKLASS')['line_length'].sum() #summarize the lenght of ditch lines within each wetland (right refers to the right geodataframe in the intersect)"
      ],
      "metadata": {
        "id": "2TAP9Z9J-j6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.barplot(x=summary.index, y=summary.values)\n",
        "plt.xlabel('NVKLASS')\n",
        "plt.ylabel('Length of Ditch Lines in meter')\n",
        "plt.title('Length of Ditch Lines within Wetlands by NVKLASS')\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4RVs2Dv-FmPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This does not tell us much since we dont take the size of each wetland into account. Lets extract the area of each wetland and devide the ditch lenght by wetland area."
      ],
      "metadata": {
        "id": "oxbRbOWYJKQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "wetlands['wetland_area'] = wetlands.geometry.area\n",
        "joined_gdf = gpd.sjoin(ditches, wetlands, how=\"inner\", op='intersects')\n",
        "joined_gdf['line_length'] = joined_gdf.geometry.length\n",
        "joined_gdf['length_per_unit_area'] = joined_gdf['line_length'] / joined_gdf['wetland_area']\n",
        "\n",
        "summary = joined_gdf.groupby('NVKLASS')['length_per_unit_area'].sum()\n",
        "summary = summary.reset_index()"
      ],
      "metadata": {
        "id": "vkW2HXCfFmAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(x='NVKLASS', y='length_per_unit_area', data=summary)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7B7Zt7G0Iv8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Buffers\n",
        "Creating a buffer around features is also a very common tool for geospatial analysis."
      ],
      "metadata": {
        "id": "YOXFT-btPerg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "\n",
        "wetlands_buffer = wetlands.geometry.buffer(500) # buffer wetlands with 500 meters\n",
        "wetlands_buffer_gdf = gpd.GeoDataFrame(geometry=wetlands_buffer) # create a new geodataframe from the buffered geometries\n",
        "\n",
        "wetlands_buffer_gdf.explore()"
      ],
      "metadata": {
        "id": "ILiqASa6LLj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you look at the map you can see that some polygons are overlapping each other. Sometimes this is not what you want and in those cases they can be merged by dissolving the overlap."
      ],
      "metadata": {
        "id": "RlrzpcDPZw-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "\n",
        "wetlands_buffer = wetlands.buffer(500)\n",
        "singlepart = wetlands_buffer.unary_union # Dissolve overlapping buffers\n",
        "singlepart = gpd.GeoDataFrame(geometry=[singlepart])\n",
        "singlepart.crs = wetlands.crs # set crs to the new data\n",
        "singlepart.explore()"
      ],
      "metadata": {
        "id": "0yz43Zj3RrCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4\n",
        "Lets say we want to restore some drained wetlands but at the same time we dont want to destroy some cultural sites nearby. We can help with this by finding drained wetlands that do not have any cultural sites within 300 meters.  Download water-related cultural sites from [Länsstyrelsen](https://ext-geodatakatalog.lansstyrelsen.se/GeodataKatalogen/srv/swe/catalog.search#/search?isTemplate=n&referenceDateRange=%7B%22range%22:%7B%22referenceDateRange%22:%7B%22gte%22:null,%22lte%22:null,%22relation%22:%22within%22%7D%7D%7D&metadataDateRange=%7B%22range%22:%7B%22metadataDateRange%22:%7B%22gte%22:null,%22lte%22:null,%22relation%22:%22within%22%7D%7D%7D&metadataAndReferenceDateRange=%7B%22range%22:%7B%22metadataAndReferenceDateRange%22:%7B%22gte%22:null,%22lte%22:null,%22relation%22:%22within%22%7D%7D%7D&sortBy=relevance&sortOrder=&query_string=%7B%22spatialRepresentationType%22:%7B%22vector-point%22:true%7D,%22ownerOrgName%22:%7B%22L%C3%A4nsstyrelsen%20Blekinge%20l%C3%A4n%22:true%7D%7D&from=1&to=30) and select wetlands with some nature values (Vissa naturvärden) that are more than 300 meters away from cultural sites. Save the selected wetlands as a geopackage."
      ],
      "metadata": {
        "id": "jL0fmuCofiqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can download the data using urllib\n",
        "from urllib.request import urlretrieve\n",
        "url = ('https://ext-dokument.lansstyrelsen.se/gemensamt/geodata/ShapeExport/Lstk.Inventerade_vattenanknutna_kulturmiljoer.zip')\n",
        "filename = '/content/Lstk.Inventerade_vattenanknutna_kulturmiljoer.zip' # you need to adjust this path on your own computer if you are using anaconda.\n",
        "urlretrieve(url, filename)\n",
        "!unzip -o '/content/Lstk.Inventerade_vattenanknutna_kulturmiljoer.zip' -d /content/culture"
      ],
      "metadata": {
        "id": "qZEofEL8hr36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data aggregation\n",
        "There is a global geospatial index based around nested hexagon polygons which is called H3. It was developed by [Uber](https://www.uber.com/en-SE/blog/h3/) and later made into open source for everyone to use.\n",
        "\n",
        "* H3 is a hierarchical geospatial index.\n",
        "* H3 was developed to address the challenges of Uber's data science needs.\n",
        "* H3 can be used to join disparate data sets.\n",
        "* In addition to the benefits of the hexagonal grid shape, H3 includes features for modeling flow.\n",
        "* H3 is well suited to apply machine learning to geospatial data.\n",
        "\n",
        "H3 can be very useful if you are working in an area and need to aggreagate multiple data soruces. Since the polygons have uniqe IDs and are constant for everyone you can easily share data with other people based on the H3 index.\n",
        "\n",
        "Also, [hexagons are the bestagons](https://www.youtube.com/watch?v=thOifuHs6eY)"
      ],
      "metadata": {
        "id": "1AV7ShJxiFYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tobler\n",
        "!pip install h3\n",
        "!pip install h3pandas"
      ],
      "metadata": {
        "id": "aST4bN18iNPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we need to select which geographical area we want to work with, start by downloading some Swedish regions from [SCB}(https://www.scb.se/hitta-statistik/regional-statistik-och-kartor/regionala-indelningar/digitala-granser/)\n"
      ],
      "metadata": {
        "id": "FGkxpgxu0_m6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlretrieve\n",
        "url = ('https://www.scb.se/contentassets/3443fea3fa6640f7a57ea15d9a372d33/shape_svenska_240104.zip')\n",
        "filename = '/content/Sweden_shapefile.zip' # you need to adjust this path on your own computer if you are using anaconda.\n",
        "urlretrieve(url, filename)\n",
        "!unzip -o /content/Sweden_shapefile.zip -d /content/sweden\n",
        "!unzip -o /content/sweden/LanSweref99TM.zip -d /content/sweden/lan"
      ],
      "metadata": {
        "id": "q4-BF09n6sl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can keep working with Blekinge since its so small and we already have some data for it."
      ],
      "metadata": {
        "id": "H6a65pkn9aOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sweden = gpd.read_file('/content/sweden/lan/Lan_Sweref99TM_region.shp')\n",
        "blekinge = sweden[sweden[\"LnNamn\"] == \"Blekinge\"] # Skogsdike is Swedish for forest ditch\n",
        "blekinge.plot()"
      ],
      "metadata": {
        "id": "YMGUyfVa7IWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this polygon we can now call on the H3 api and create a hexagon index for Blekine. There are multiple resoltuions where the bigest hexagon is 4,357,449.41 km² and the smallest 0.895 m²\n",
        "\n",
        "The resolution parameter goes from 0 to 15 but due to limited RAM on colab we will use 7 here."
      ],
      "metadata": {
        "id": "j7nwVS8G9_qF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import h3pandas\n",
        "import tobler\n",
        "blekinge_hexagons = tobler.util.h3fy(blekinge, resolution=7, clip=False, buffer=False, return_geoms=True)\n",
        "blekinge_hexagons.explore()"
      ],
      "metadata": {
        "id": "ejd2mYae1s1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note the hex_id column. Different resolutions have different IDs but each polygon ID is uniqe which means that you can use it to join and share data based on this global ID."
      ],
      "metadata": {
        "id": "rEWlu-coAIS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blekinge_hexagons"
      ],
      "metadata": {
        "id": "aPb0vnBS2Hyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets summarize the number of ditch lines within each hexagon."
      ],
      "metadata": {
        "id": "7eX1OwsjEq1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "\n",
        "# make sure that the coordinate system is the same\n",
        "ditches  = ditches.to_crs(blekinge_hexagons.crs)\n",
        "\n",
        "# Perform a spatial join\n",
        "joined = gpd.sjoin(blekinge_hexagons, ditches, op='intersects', how='left')\n",
        "\n",
        "# Group by hex_id and count the number of sites within each hexagon\n",
        "summary = joined.groupby('hex_id').size().reset_index(name='ditch_count')\n",
        "\n",
        "# Merge the summary with the hexagon GeoDataFrame to add the cnumber of sites within each hexagon\n",
        "hexagons_with_sites = blekinge_hexagons.merge(summary, on='hex_id', how='left')\n",
        "hexagons_with_sites.explore('ditch_count')"
      ],
      "metadata": {
        "id": "VkPg2RoUAvD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, since some ditches are longer than others it makes more sense to summarize the length of ditches instead of the number of line segments."
      ],
      "metadata": {
        "id": "h5A-BuL-E0Kr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "\n",
        "ditches = ditches.to_crs(blekinge_hexagons.crs)\n",
        "joined = gpd.sjoin(blekinge_hexagons, ditches, op='intersects', how='left')\n",
        "\n",
        "# Calculate the length of each ditch line\n",
        "joined['line_length'] = joined.geometry.length\n",
        "\n",
        "# Group by hex_id and sum the length of ditch lines within each hexagon\n",
        "summary = joined.groupby('hex_id')['line_length'].sum().reset_index()\n",
        "hexagons_with_ditch_length = blekinge_hexagons.merge(summary, on='hex_id', how='left')\n",
        "hexagons_with_ditch_length.explore('line_length')"
      ],
      "metadata": {
        "id": "2xNkf_ITDC8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we aggregate more data to the hexagons. For example, lets also add the area of wetlands within each hexagon."
      ],
      "metadata": {
        "id": "6JHSDiMiFiC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "wetlands = wetlands.to_crs(hexagons_with_ditch_length.crs)\n",
        "joined_wetlands = gpd.sjoin(hexagons_with_ditch_length, wetlands, op='intersects', how='left')\n",
        "\n",
        "# Calculate the area of each wetland polygon\n",
        "joined_wetlands['wetland_area'] = joined_wetlands.geometry.area\n",
        "\n",
        "# Group by hex_id and sum the area of wetlands within each hexagon\n",
        "summary_wetlands = joined_wetlands.groupby('hex_id')['wetland_area'].sum().reset_index()\n",
        "\n",
        "hexagons_with_both_ditch_lenght_and_wetland_area = hexagons_with_ditch_length.merge(summary_wetlands, on='hex_id', how='left')\n",
        "hexagons_with_both_ditch_lenght_and_wetland_area"
      ],
      "metadata": {
        "id": "OjzbAWDwEN4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 5\n",
        "Add the number of cultural sites to these hexagons so you get ditch length, wetlands area and number of cultural sites within the same geodataframe."
      ],
      "metadata": {
        "id": "7Rch1klKGact"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9cXBwoAyH29d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusions\n",
        "Geopandas is a powerful tool to wrangle geospatial vector data and can read and write multiple different file formats. Aggrigating data is very useful for further analyses and we will use this skill in the next module to combine aggregated vector data with machine learning and implement the model to predict things on the H3 geospatial index. Here is an example of how it can be used in the real world: https://beta.behovskartan.se/"
      ],
      "metadata": {
        "id": "LIFd5TFAGtDp"
      }
    }
  ]
}