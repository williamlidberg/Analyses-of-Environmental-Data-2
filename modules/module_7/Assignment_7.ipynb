{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMl6QsddczIPzxNprk0c47x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/williamlidberg/Analyses-of-Environmental-Data-2/blob/main/modules/module_7/Assignment_7_machine_learning_on_vector_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine learning\n",
        "The core idea of machine learning is to program a system using data instead of with code. There are multiple types of machine learning which are used for different types of data such as images, text or numbers, but there are two main subdomains: traditional machine learning and deep learning. Machine learning was introduced around 1940 and deep learning 1963 but it was mainly deep learning that exploded in the 2000s. Deep learning is great but it is not always the best solution, especially when working with tablular data (tables with rows and columns). Therefore, this module will introduce you to traditional machine learning on geopandas dataframes.\n",
        "\n",
        "## Random forest\n",
        "Random forests is a very robust machine learning method which I both love and hate. I love it becourse it is so easy to use and always provide a good baseline. I hate it becouse it is a bit booring.\n",
        "\n",
        " Random forest can be used for both classification and regression and works by building many decicion trees on randomly selected parts of your dataframe. Multiple deicion trees makes a decision forest hence the name random forest."
      ],
      "metadata": {
        "id": "48d6zNMNnNuZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9ipcFgQmbNH"
      },
      "outputs": [],
      "source": [
        "!pip install geopandas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aquire some data\n",
        "\n",
        "As always we need to get our hands on some data. We will start by reusing some data sources from previous modules. In our first example we will use Wetlands from Uppsala.\n"
      ],
      "metadata": {
        "id": "oG__AfY2TvvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download wetlands\n",
        "This will give us three shapefiles and a pdf describing the data. We are mainly interested in VMI_C_Objekt_KLAR_2022 and VMI_C_Vatten_TOT_2022. The first contains the wetland polygons and the second contains polygons of water bodies within each wetland. VMI_C_Objekt_KLAR_2022 also contains our target variable which is the nature value classification of each wetland. We want to build a model that can predict the nature value of wetlands based on other data."
      ],
      "metadata": {
        "id": "fD-AesWk4DtH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlretrieve\n",
        "url = ('https://geodata.naturvardsverket.se/nedladdning/VMI/C_Uppsala/5_GIS_skikt/VMI_C_GIS_2022.zip')\n",
        "filename = '/content/VMI_C_GIS_2022.zip' # you need to adjust this path on your own computer if you are using anaconda.\n",
        "urlretrieve(url, filename)\n",
        "!unzip -o /content/VMI_C_GIS_2022.zip -d /content/wetlands"
      ],
      "metadata": {
        "id": "5WXVJPGVi5mJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download ditches\n",
        "These are the same ditches as before. They were detected using deep learning on airborne laser data."
      ],
      "metadata": {
        "id": "6ZYrjUUfz0aV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = ('https://geodata.naturvardsverket.se/nedladdning/Diken/Diken_Sverige/Diken_lansvis/Diken_C.zip')\n",
        "filename = '/content/Diken_C.zip' # you need to adjust this path on your own computer if you are using anaconda.\n",
        "urlretrieve(url, filename)\n",
        "!unzip -o '/content/Diken_C.zip' -d /content/ditches"
      ],
      "metadata": {
        "id": "Ki2DhgDwkEKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download valued forest areas\n",
        "These are key habitats that were judged to be extra valuable during field visits."
      ],
      "metadata": {
        "id": "UAo6peq_4Hvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = ('https://geodata.naturvardsverket.se/nedladdning/skogliga_vardekarnor_2016.zip')\n",
        "filename = '/content/skogliga_vardekarnor_2016.zip' # you need to adjust this path on your own computer if you are using anaconda.\n",
        "urlretrieve(url, filename)\n",
        "!unzip -o '/content/skogliga_vardekarnor_2016.zip' -d /content/valued_forest"
      ],
      "metadata": {
        "id": "VGgxvWvA4LA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download waterbodies as polygons\n",
        "These are polygons of both lakes and rivers."
      ],
      "metadata": {
        "id": "G2nif8TkhPEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlretrieve\n",
        "url = ('https://opendata-download.smhi.se/svar/Vattenytor_2016.zip')\n",
        "filename = '/content/Vattenytor_2016.zip' # you need to adjust this path on your own computer if you are using anaconda.\n",
        "urlretrieve(url, filename)\n",
        "!unzip -o '/content/Vattenytor_2016.zip' -d /content/waterbodies\n",
        "\n"
      ],
      "metadata": {
        "id": "uK3pkXWwhTf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read all data into geodataframes\n",
        "Due to the large amounts of data this can take a while (~10 min)."
      ],
      "metadata": {
        "id": "0aTRfoLV0WMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "\n",
        "wetlands = gpd.read_file('/content/wetlands/VMI_C_Objekt_KLAR_2022.shp', crs='EPSG:3006') # These contain the targer variable.\n",
        "waterbodies = gpd.read_file('/content/wetlands/VMI_C_Vatten_TOT_2022.shp')\n",
        "waterbodies = waterbodies.to_crs('EPSG:3006')\n",
        "ditches = gpd.read_file('/content/ditches/Diken_C.gpkg', crs='EPSG:3006') # Note that this is a geopackage\n",
        "ditches= ditches.to_crs('EPSG:3006')\n",
        "forest = gpd.read_file('/content/valued_forest/Skogliga_vardekarnor.shp', crs='EPSG:3006')\n",
        "\n",
        "lakes = gpd.read_file('/content/waterbodies/Vattenytor_2016.shp', crs='EPSG:3006')"
      ],
      "metadata": {
        "id": "qBKRuS2TxU9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "you can inspect the size of a geodataframe by using the info() command."
      ],
      "metadata": {
        "id": "XMd3DAunqJbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ditches.info()"
      ],
      "metadata": {
        "id": "DgK4qLlKp2yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combine the data\n",
        "The next step is to combine the data into a single dataframe using a series of joins and spatial relates from geopandas. We want a final dataframe containing the following:\n",
        "\n",
        "\n",
        "*   The lenght of ditches within each wetland\n",
        "*   Wetland area\n",
        "*   Area of wetland waterbodies\n",
        "*   Distance to nearest lake or river\n",
        "*   Distance to valuable forests\n",
        "\n"
      ],
      "metadata": {
        "id": "fC84LqeP-PGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wetlands # Its the column Nv_klass_G that we want to predict."
      ],
      "metadata": {
        "id": "2Jf8iRMmq7GW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start by droping columns we dont need to make it easier to follow the process."
      ],
      "metadata": {
        "id": "6-PxMEhHq5YY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_keep = ['N1_LOID', 'Nv_klass_G', 'geometry']\n",
        "wetlands = wetlands[columns_to_keep] # Drop columns not in columns_to_keep\n",
        "wetlands"
      ],
      "metadata": {
        "id": "P6VX7hguq-7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intersect the ditch lines with the wetland polygons and calculate the lenght of ditch channels within each wetland."
      ],
      "metadata": {
        "id": "flA2O613AhEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "intersect = gpd.overlay(ditches, wetlands, how='intersection')\n",
        "intersect['ditch_length'] = intersect.geometry.length # Calculate length of intersecting ditch lines\n",
        "wetland_ditch_length = intersect.groupby('N1_LOID')['ditch_length'].sum().reset_index() # Aggregate lengths by wetland polygon\n",
        "wetlands_with_ditch_length = wetlands.merge(wetland_ditch_length, on='N1_LOID', how='left').fillna({'ditch_length': 0})\n",
        "wetlands_with_ditch_length"
      ],
      "metadata": {
        "id": "M9lMww3U8dH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intersect the wetland waterbodies polygons with the wetland polygons and calculate the waterbody area within each wetland."
      ],
      "metadata": {
        "id": "VmAUrZrkI3_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "\n",
        "intersect = gpd.overlay(waterbodies, wetlands, how='intersection')\n",
        "intersect['water_area'] = intersect.geometry.area # Calculate water area for intersecting waterbodies\n",
        "area_waterbodies = intersect.groupby('N1_LOID')['water_area'].sum().reset_index()\n",
        "wetlands_with_waterbodies = wetlands.merge(area_waterbodies, on='N1_LOID', how='left').fillna({'water_area': 0})\n",
        "wetlands_with_waterbodies"
      ],
      "metadata": {
        "id": "EbzVQ1CVyAFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For lakes and valuable forests we need to calculate the distance to nearest lake instead of intersecting wetlands and lakes.\n",
        "\n",
        "Start with lakes"
      ],
      "metadata": {
        "id": "pbXlRgB13ENL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "from scipy.spatial import cKDTree\n",
        "\n",
        "wetlands_with_lake_distance = wetlands.copy()\n",
        "lake_spatial_index = cKDTree(lakes.centroid.apply(lambda x: (x.x, x.y)).tolist()) # Create a spatial index for the lake centroids\n",
        "\n",
        "# Function to calculate the nearest distance\n",
        "def nearest_distance(point):\n",
        "    distance, idx = lake_spatial_index.query((point.x, point.y))\n",
        "    return distance\n",
        "\n",
        "wetlands_with_lake_distance['nearest_lake_distance'] = wetlands_with_lake_distance.centroid.apply(nearest_distance) # Apply the function to each wetland centroid to get the nearest distance to a lake\n",
        "wetlands_with_lake_distance"
      ],
      "metadata": {
        "id": "adNILAIWxHtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then do the same with valuable forests"
      ],
      "metadata": {
        "id": "IrYX6k89lrtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "from scipy.spatial import cKDTree\n",
        "\n",
        "wetlands_with_forest_distance = wetlands.copy()\n",
        "forest_spatial_index = cKDTree(forest.centroid.apply(lambda x: (x.x, x.y)).tolist()) # Create a spatial index for the lake centroids\n",
        "\n",
        "# Function to calculate the nearest distance\n",
        "def nearest_forest_distance(point):\n",
        "    distance, idx = forest_spatial_index.query((point.x, point.y))\n",
        "    return distance\n",
        "\n",
        "wetlands_with_forest_distance['nearest_forest_distance'] = wetlands_with_forest_distance.centroid.apply(nearest_forest_distance) # Apply the function to each wetland centroid to get the nearest distance to a lake\n",
        "wetlands_with_forest_distance"
      ],
      "metadata": {
        "id": "KoMcpDxY9ZjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally you need to join all geodataframes. The data can be joined based on the attribute 'N1_LOID'."
      ],
      "metadata": {
        "id": "cmwu3Mx_nlwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "attribute_dataframes = [wetlands_with_ditch_length, wetlands_with_waterbodies, wetlands_with_lake_distance, wetlands_with_forest_distance]\n",
        "merged_wetlands = wetlands.copy()\n",
        "\n",
        "# Merge the dataframes one by one based on N1_LOID\n",
        "for df in attribute_dataframes:\n",
        "    cols_to_use = df.columns.difference(merged_wetlands.columns)\n",
        "    merged_wetlands = pd.merge(merged_wetlands, df[cols_to_use], left_index=True, right_index=True, how='outer')\n",
        "merged_wetlands"
      ],
      "metadata": {
        "id": "kOQcvSOl3I89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final step is to drop the column for the ID and geometry so the machine learning model does not attempt to train on those attributes."
      ],
      "metadata": {
        "id": "sHMRle9vATLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_data = merged_wetlands.drop(['geometry', 'N1_LOID'], axis=1)\n",
        "clean_data = clean_data.reset_index(drop=True)\n",
        "clean_data"
      ],
      "metadata": {
        "id": "puHS6b5WAa3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine learning\n",
        "Now when we have a dataframe with wetlands and some attributes we can use them to train a model that can predict the nature value on mires.\n",
        "\n",
        "We will use [sklearn](https://scikit-learn.org/stable/) to build and test a basic random forest model. To evaluate weather the model is good or not we will split the data into training 80% and testing 20%."
      ],
      "metadata": {
        "id": "7zkrlxRQoW2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "y = clean_data.iloc[:,0] # This is Nv_klass_G\n",
        "x = clean_data.iloc[:,1:] # These are all the other attributes variables\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=0, stratify = y) # splits the data into training and testing data. test_size=0.2 means that 20% of the wetlands will be set aside for testing.\n",
        ""
      ],
      "metadata": {
        "id": "UKINCm_r5XLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision tree\n",
        "It is always good to start with a simple model so we will build a decision tree using or training data and test it on our test data."
      ],
      "metadata": {
        "id": "I3pLL7SVGuJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "clf = DecisionTreeClassifier(max_depth=3) # This number determains how many decisions the tree will contain\n",
        "clf.fit(x_train, y_train)\n"
      ],
      "metadata": {
        "id": "t3NsXiFLG6eR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also inspect the trained decision tree to see whats going on."
      ],
      "metadata": {
        "id": "S2eLc-vDH2CN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "plot_tree(clf, feature_names=x.columns, class_names=clf.classes_, filled=True, fontsize=8)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xNDjqKc9HyAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first decision is on the lenght of ditch channels on a wetland.\n",
        "\n",
        "The standard way to evaluate a machine learning model is to use it to predict the test data and then compare the prediction to the test labels."
      ],
      "metadata": {
        "id": "7qGB8ElBHRgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "y_pred = clf.predict(x_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "IY8QIRTRHUiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy is between 0 and 1 where 1 is 100% and something like 0.1 would be 10% accurate. In other words, the model would predict the right nature value class 10% of the time. For more detailed information we can use a classification report to see how the model preforms on different classes. The classification report produces values for precision, recall, f1-score and support. Lets break those down a bit. using class 1 (Mkt högt natuvärde/very high nature value).\n",
        "\n",
        "\n",
        "*   Precision = % of how many of the predicted wetlands in class 1 that were actually in class 1.\n",
        "*   Recall = % of all wetlands in class 1 that the model predicted as class 1.\n",
        "*   f1-score = A combination of precision and recall. If your model has high precision but low recall, or vice versa, the F1-score will be lower. Higher number is better.\n",
        "*   support = How many samples were in that class in the test data.\n",
        "\n",
        "\n",
        "This might be alot to take in so focus on the f1-score for now. we want as high f1-score as possible. You can also run the prediction on the same data that it was trained on to compare that with the prediction on the test data. If the difference is big then your model has overfitted to the training data.\n"
      ],
      "metadata": {
        "id": "u3-71L8hHk3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test the model on its own training data\n",
        "from sklearn.metrics import accuracy_score\n",
        "y_pred = clf.predict(x_train)\n",
        "accuracy = accuracy_score(y_train, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "orNh2Qwg7DKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred_test = clf.predict(x_test)\n",
        "print(classification_report(y_test, y_pred_test, zero_division=0))"
      ],
      "metadata": {
        "id": "MFwCx6Qn0HBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another way to dive deeper into the model output is to look at a confusion matrix where each prediction is compared to the actual class of that wetland. Each square will show the number of correct predictions.\n"
      ],
      "metadata": {
        "id": "0jaakk4F16Ct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_pred = clf.predict(x_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix\n",
        "# Get the class names\n",
        "class_names = ['1. Mkt högt naturvärde', '2. Högt naturvärde ', '3. Visst naturvärde', '4. Lågt naturvärde']  # Replace with your class names\n",
        "\n",
        "# Visualize the confusion matrix with class names\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tz6va6bU0MqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1\n",
        "Change the tree depth to see if that improves the accuracy of the model. Inspect the f1-score and confusion matrix."
      ],
      "metadata": {
        "id": "Yj6wQXM4M5Av"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision forest\n",
        "Now when you have seen a decision tree it's time to build a decision forest. It is the same thing but with more trees working together. The parameter in the code below \"n_estimators=2\" determains the number of trees in the forest. In this case it will train an ensabmle of 2 trees."
      ],
      "metadata": {
        "id": "Av5xc538C2Ue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=200, max_depth=3) # create a random forest model\n",
        "rf_clf.fit(x_train, y_train) # train the model"
      ],
      "metadata": {
        "id": "5tNOrhZWC-yJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = rf_clf.predict(x_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "_pqRV1p3DLgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also inspect the decision trees in a random forest but if the forest contains alot of trees this can be a bit tedious."
      ],
      "metadata": {
        "id": "-ff6YgysNe0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import export_graphviz\n",
        "import pydotplus\n",
        "from IPython.display import Image\n",
        "\n",
        "tree_index = 0  # Change this to select a different tree. Inspect the second tree in the forest by changing 0 to 1.\n",
        "tree = rf_clf.estimators_[tree_index]\n",
        "\n",
        "dot_data = export_graphviz(tree, out_file=None,\n",
        "                           feature_names=x.columns,\n",
        "                           class_names=clf.classes_,\n",
        "                           filled=True, rounded=True)\n",
        "\n",
        "# Create a Graphviz object\n",
        "graph = pydotplus.graph_from_dot_data(dot_data)\n",
        "Image(graph.create_png())"
      ],
      "metadata": {
        "id": "RJAIlWxbNeSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With random forest we can also get an idea of what attributes are important for the model using feature importance. Keep in mind that machine learning is a bit of a black box so in order to see why a feature is important you would have to inspect all the decision trees in a random forest model."
      ],
      "metadata": {
        "id": "X5eCusrD453o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get feature importances from the trained model\n",
        "feature_importance = rf_clf.feature_importances_\n",
        "feature_names = x.columns\n",
        "indices = feature_importance.argsort()[::-1]\n",
        "\n",
        "# create a dataframe for nicer plot\n",
        "importance_df = pd.DataFrame({'Feature': feature_names[indices],\n",
        "                              'Importance Score': feature_importance[indices]})\n",
        "\n",
        "# higher values are more important for the model\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Importance Score', y='Feature', data=importance_df, hue='Feature', palette='viridis', legend=False)\n",
        "plt.title(\"Feature Importances\")\n",
        "plt.xlabel(\"Importance Score\")\n",
        "plt.ylabel(\"Features\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "keErHczg5HTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2\n",
        "Train a random forest model with more trees and see if that improves the result. Can you think of a reason to not build as many trees as possible?\n"
      ],
      "metadata": {
        "id": "MELVt3iNMyP9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement your model\n",
        "You have now trained a model and evaluated the preformance of the model. The next step is to combine the prediction with the original dataframe. In this case the geodataframe with all the combined data \"merged_wetlands\". Note that we want to keep the geometry and ID so we can save the result as a geopackage or shapefile."
      ],
      "metadata": {
        "id": "kCIR7yp86QM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "subset_df = merged_wetlands[x_train.columns] # Subset the DataFrame to include only the columns used for training\n",
        "predictions = rf_clf.predict(subset_df) # run the prediction\n",
        "predictions_df = pd.DataFrame(predictions, columns=['Prediction']) # Convert predictions to a DataFrame\n",
        "final_df = pd.concat([merged_wetlands, predictions_df], axis=1) # merge predictions with the original geodataframe\n",
        "final_df.to_file('/content/predicted_wetlands.gpkg', driver='GPKG')"
      ],
      "metadata": {
        "id": "QIQYUc-08cLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize the result"
      ],
      "metadata": {
        "id": "DFI8NCzsFonu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'final_df' is your GeoDataFrame and 'Prediction' and 'Nv_klass_G' are the columns you want to use for coloring\n",
        "\n",
        "# Plot the first GeoDataFrame (colored by 'Prediction') on the first axis\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
        "\n",
        "# Plot the second GeoDataFrame (colored by 'Nv_klass_G') on the second axis\n",
        "final_df.plot(column='Nv_klass_G', cmap='viridis', ax=ax1, legend=True)\n",
        "ax2.set_title('Original nature value')\n",
        "\n",
        "final_df.plot(column='Prediction', cmap='viridis', ax=ax2, legend=True)\n",
        "ax1.set_title('Predicted nature value')\n",
        "\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "N5kuPNBoB4_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3\n",
        "Start from the dataframe merged_wetlands and calculate wetland area and perimeter. Then devide area by perimeter to get wetland shape complexity and finally train a new random forest model with the three new variables included."
      ],
      "metadata": {
        "id": "5FAogvcRFrpF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Neural networks\n",
        "\n",
        "Neural networks are designed to mimic the behavior of the human brain. It consists of interconnected nodes called neurons, organized into layers. Each neuron receives input, processes it, and passes the output to other neurons.\n",
        "\n",
        "*    Input layer: This layer receives the initial data or features that are fed into the neural network. Each neuron in the input layer represents a feature of the input data.\n",
        "\n",
        "*    Hidden layers: These layers sit between the input and output layers. They perform computations on the input data using weighted connections between neurons. Hidden layers enable the neural network to learn complex patterns and relationships within the data.\n",
        "\n",
        "*    Output layer: This layer produces the final output of the neural network. The number of neurons in the output layer depends on the task the network is designed for. For example, in a classification task, each neuron in the output layer may represent a different class, in our case its wetland classification.\n",
        "\n",
        "We can build simple neural networks using sklearn. Lets start by building a neural network with one hidden layer with five neurons."
      ],
      "metadata": {
        "id": "Rt2DGIsUDoki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# define and train the model hidden_layer_sizes=(5) means that the hidden layer will have 5 neurons.\n",
        "nnet = MLPClassifier(solver='lbfgs', max_iter=1000, hidden_layer_sizes=(5), random_state=1)\n",
        "nnet.fit(x_train, y_train)\n",
        "\n",
        "# evaluate the trained model on test data\n",
        "y_pred = nnet.predict(x_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "Id_cX26u8XXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To better understand what this is we can install a small package for visualization."
      ],
      "metadata": {
        "id": "tN4EqsHfF2Lt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nnv"
      ],
      "metadata": {
        "id": "78g0fIRrDKuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets plot the neural network we just trained. It has four input nodes and four output nodes, one for each wetland class. It only have one hidden layer with five nodes. The difference between machine learning and deep learning is how many hidden layers the model has."
      ],
      "metadata": {
        "id": "HPrnE7YrG7GS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"figure.figsize\"] = (5,5)\n",
        "from nnv import NNV\n",
        "\n",
        "layersList = [\n",
        "    {\"title\":\"input\\n\", \"units\": 4},\n",
        "    {\"title\":\"hidden 1\\n\", \"units\": 5}, # This is the hidden layer\n",
        "    {\"title\":\"output\\n\", \"units\": 4,},\n",
        "]\n",
        "\n",
        "NNV(layersList, max_num_nodes_visible=10,font_size=11).render()"
      ],
      "metadata": {
        "id": "iTQf6XFrDNlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets build a deep neural network using two hidden layers."
      ],
      "metadata": {
        "id": "4aUY1PpIHXyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nnet = MLPClassifier(solver='lbfgs', max_iter=1000, hidden_layer_sizes=(5,5), random_state=1)\n",
        "nnet.fit(x_train, y_train)\n",
        "\n",
        "# evaluate the trained model on test data\n",
        "y_pred = nnet.predict(x_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "OKCgpk0JHcuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is how the new neural network looks"
      ],
      "metadata": {
        "id": "gOvYBI6KH4IA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"figure.figsize\"] = (5,5)\n",
        "from nnv import NNV\n",
        "\n",
        "layersList = [\n",
        "    {\"title\":\"input\\n\", \"units\": 4},\n",
        "    {\"title\":\"hidden 1\\n\", \"units\": 5},\n",
        "    {\"title\":\"hidden 2\\n\", \"units\": 5},\n",
        "    {\"title\":\"output\\n\", \"units\": 4,},\n",
        "]\n",
        "\n",
        "NNV(layersList, max_num_nodes_visible=10,font_size=11).render()"
      ],
      "metadata": {
        "id": "Agq4qHC7H6sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 4\n",
        "Our dataset is not optimal for neural networks since its quite limited and we have not rescaled any of our attributes. Play around with the number of neurons and hidden layers and see if you can get better accuracy than with the basic decision tree model."
      ],
      "metadata": {
        "id": "Hzqs9ISkNdl4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "Machine learning is a great tool to quickly build predictive models. With vector data we can calculate spatial relations such as distance and area using geopandas. Since machine learning readily can be applied to attribute tables we can combine geopandas with machine learning for predictions.\n",
        "\n",
        "One strenght of decision trees and random forest is that they do not expect or require the data to be normaly distributed. You can even mix classified data such as land use with continues values such as distances.\n",
        "\n",
        "We will work more with deep learning in module nine but generally a random forest is much easier to work with and often produce similar results on tabular data.\n",
        "\n",
        "Look into the sklearn documentation for more inspiration on other machine learning methods: https://scikit-learn.org/stable/user_guide.html\n"
      ],
      "metadata": {
        "id": "SpKrHR2iMBh9"
      }
    }
  ]
}